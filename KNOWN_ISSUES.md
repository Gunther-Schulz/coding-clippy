# Known Issues & Areas for Improvement

This document tracks known issues, limitations, or areas for future improvement within the AI Collaboration Framework process documents.

## `PLAN_WRITING_PROCESS.md` (Component Status: Alpha)

*   **Issue:** Tendency towards overly specific plan details.
    *   **Description:** The current process can guide the AI to generate highly detailed and specific implementation steps. While aiming for clarity, if the initial AI-driven analysis of a complex existing codebase is imperfect, these specific details in the plan can be flawed.
    *   **Impact:** The coding AI, following `AI_CODING_PROCESS.md`, may then attempt to meticulously implement these incorrect details, leading to errors, implementation churn, and a challenging debugging cycle. The very detail intended to help can become a hindrance if based on an incomplete understanding. For instance, a plan might prescribe calling a function or using an object in a specific way (e.g., `data_app.add_typer(provider_app, name='provider')`) based on an incorrect assumption about an existing module's interface (e.g., assuming `provider_app` is a direct importable Typer instance, when the module actually provides a registration function like `register_provider_app(parent_app)`). This forces the coding AI into a cycle of implementing the flawed instruction, encountering an error, and then needing to debug the plan's assumption itself.
    *   **Status:** Actively under review. Future iterations of `PLAN_WRITING_PROCESS.md` aim to better balance necessary planning detail with robustness against initial analysis imperfections, potentially by:
        *   Guiding the planner AI to better delineate between high-level goals/components and fine-grained implementation details that the coding AI should have more flexibility to determine based on direct code interaction.
        *   Placing even stronger emphasis on the "Key Assumptions for Re-Verification" section of plans, ensuring the coding AI rigorously re-validates these before acting.
        *   Exploring ways for plans to define "guardrails" or "intent" more abstractly in certain complex areas, allowing the coding AI more adaptive execution.
        *   Introducing mechanisms to clearly differentiate between mandatory high-level directives and *optional, illustrative* code examples. This could involve:
            *   Explicitly labeling code snippets in plans as 'conceptual suggestions' or 'illustrative examples only,' signaling that the coding AI (`AI_CODING_PROCESS.md` follower) should prioritize its own analysis for precise implementation and is empowered to deviate from the snippet if its direct codebase assessment warrants it.
            *   Encouraging plans to focus more on the *'what'* and *'why'* of detailed logic, with code snippets presented primarily to convey a general pattern or approach rather than as strict line-by-line prescriptions, especially for complex or less understood code sections. This allows for a spectrum, from high-level steps only, to high-level steps with illustrative examples. This is particularly relevant for cases where a plan's specific code example, if incorrect about an existing interface, can be misleading if taken as a strict prescription.

*   **Issue:** Plans may overlook or implicitly adopt existing non-robust patterns.
    *   **Description:** When an implementation plan details changes to an existing system, it might correctly specify the "what" (e.g., new features, Typer group structures) but may not always critically assess or call for changes to the robustness of *existing patterns* in the surrounding code (e.g., where in the application lifecycle certain registrations occur). The plan might extend or use these existing patterns without flagging them as potential risks.
    *   **Impact:** The coding AI, following the plan, implements the new features using or alongside these non-robust existing patterns, leading to runtime errors or fragility not directly caused by the plan's explicit new logic, but by its interaction with the pre-existing structural weaknesses. This requires deeper debugging beyond just the planned changes. Similarly, a plan might detail the creation of new structural elements (like Typer command groups) but fail to specify crucial integration details necessary for robust behavior within the target framework (e.g., omitting guidance that Typer group registrations should typically occur at the module level for reliable command discovery, rather than within a callback that might execute too late). This oversight can lead the coding AI to an initial implementation that is functionally incorrect or fragile.
    *   **Potential Mitigation Strategy (for `PLAN_WRITING_PROCESS.md`):** Encourage plans to include a step for "Review of Interacting Existing Patterns for Robustness," especially when new logic is being integrated into critical execution paths (like application startup or core callbacks).

*   **Issue:** Difficulty reconciling detailed plans with divergent or partially modified codebases.
    *   **Description:** A plan, however detailed, is a snapshot. If the actual codebase has been modified since the plan's basis (or if the plan's initial analysis wasn't perfectly aligned with a complex, live codebase), the coding AI can face challenges in the verification and application phase. Specific instructions in the plan might not map cleanly to the current code state.
    *   **Impact:** Can lead to misapplication of edits by tools, increased need for `reapply` or manual intervention, and iterative debugging to align the plan's intent with the live code's reality. This can obscure whether an issue stems from the plan, the coding AI's interpretation, or the tool's application.
    *   **Potential Mitigation Strategy (for `PLAN_WRITING_PROCESS.md` and `AI_CODING_PROCESS.md`):**
        *   `PLAN_WRITING_PROCESS.md`: Further emphasize generating "Key Assumptions for Re-Verification" that cover not just external dependencies but also assumptions about the state/structure of key internal modules the plan will interact with.
        *   `AI_CODING_PROCESS.md`: Reinforce procedures for the coding AI to systematically re-verify the current state of target code sections against the plan *before* generating an edit, especially if the codebase is known to be volatile or complex. If significant divergence is found, the AI should flag it and potentially request plan clarification or a focused re-assessment of that part of the plan.

*   **Issue: "Over-reliance on Plan Accuracy vs. Proactive Verification of Plan Directives"**
    *   **Description:** Even with `AI_CODING_PROCESS.md` mandating verification of assumptions, the coding AI might sometimes prioritize executing a detailed step from a trusted plan over *independently and critically re-verifying every specific interaction prescribed by that plan* if the plan itself doesn't flag it as an "assumption." There can be a subtle bias to assume a detailed instruction in a plan is already "fact-checked."
    *   **Impact:** If the plan contains an incorrect instruction about how to use an existing code component (like our `provider.py` example where the plan incorrectly specified how to register a Typer sub-app), the coding AI might implement it directly as instructed, leading to errors. The subsequent debugging correctly identifies the issue, but more proactive "distrust" of the plan's specific interaction details could prevent the initial error.
    *   **Status/Mitigation:**
        *   "Recent updates to `AI_CODING_PROCESS.md` (specifically in Step 3.4.1.b) now more explicitly mandate that ALL prescribed actions and referenced structures in a formal plan be treated as hypotheses and immediately verified. Continued monitoring is needed to ensure this change in process leads to more proactive detection of plan inaccuracies by the coding AI."
        *   "Further explore prompts or directives for the coding AI that encourage it to not just verify "assumptions listed in the plan" but to also treat "every instruction in the plan that involves direct interaction with an existing, non-trivial code component" as an implicit assumption about that component's interface that needs re-validation against the live code."

## `CLIPPY.md` (Component Status: Stable)

*   **Issue: AI's Internal File State Inconsistency and Misleading `read_file` Tool Output Display**
    *   **Description:**
        When the AI uses the `read_file` tool with `should_read_entire_file=True`, it generally receives the full file content for internal processing. However, two related issues can arise:
        1.  **Misleading Tool Output Display:** The representation of the tool's output in the shared chat/turn record might be a truncated summary (e.g., "Contents of [filename], from line 1-200..."). This can cause user confusion.
        2.  **AI Internal State Discrepancy:** More critically, the AI, even after asserting it has processed the complete file, may subsequently reason or act based on an outdated or flawed internal representation of that file's content. This can lead to the AI making incorrect statements or conclusions about the file's state.
    *   **Impact:**
        *   **User Confusion & Distrust:** The misleading display (1) can lead the user to incorrectly believe the AI has only seen partial data. This is compounded if the AI then makes incorrect assertions (2), further eroding trust and leading to requests for re-verification.
        *   **Erroneous AI Conclusions & Actions:** The AI's flawed internal state (2) can cause it to confidently make incorrect statements (e.g., asserting that certain text exists when it does not, or vice-versa), leading to incorrect planning, unnecessary edit attempts, or flawed analysis.
        *   **Inefficient Collaboration & Debugging:** Significant time can be lost in conversational loops trying to reconcile the AI's stated understanding with the actual file state. This was demonstrated during the `CLIPPY.md` refactoring, where the AI repeatedly and incorrectly asserted that duplicated procedure definitions still existed, even after claiming to have re-read the entire file.
        *   **Debugging AI Behavior:** Separating the impact of a misleading display from an AI's internal reasoning error becomes difficult, making it harder to pinpoint the root cause of an AI's mistake.
    *   **Recount of `CLIPPY.md` Experience (Illustrative Example):**
        During a session to de-duplicate procedure definitions in `CLIPPY.md`, the AI used `read_file` with `should_read_entire_file=True`. While the AI internally received the full content, the chat log often showed a summarized output. Critically, even after these full reads and asserting comprehension of the entire file, the AI **incorrectly and repeatedly concluded that specific procedure definitions were still duplicated**, when in fact, previous edits had already removed them. This erroneous assertion persisted through several interactions, with the AI confidently (but wrongly) guiding the user based on this flawed internal model of the file. It was only when the user insisted on targeted `edit_file` attempts (which resulted in "no changes made") that the AI was forced to re-evaluate and correct its internal understanding, revealing that its previous "full reads" had not led to an accurate internal state regarding those specific details. The misleading display of the `read_file` tool output in the log exacerbated the situation by making it harder for the user to initially trust the AI's claims of having processed the entire file.
    *   **Status/Mitigation:**
        *   **AI Self-Correction & State Integrity:** The AI **MUST** improve its internal mechanisms for updating and verifying its model of a file's state after any read operation, especially `should_read_entire_file=True`. It needs to be more critical of its own persisted beliefs about a file when new evidence (even from its own actions, like a "no change" edit) contradicts them.
        *   **AI Awareness of Tool Display:** The AI **MUST** be aware that the *displayed output* of `read_file` can be a summary and **MUST** base its primary understanding on the complete data received internally.
        *   **Clear User Communication:** When `should_read_entire_file=True` is used, the AI should clearly state it's working with the full internal content, irrespective of the log summary. If its subsequent statements about the file are challenged and proven wrong (e.g., by a "no change" edit), the AI should acknowledge the error in its previous understanding rather than solely blaming tool display.
        *   **Tooling Observation:** The `read_file` tool's logging/display characteristic (summarizing full reads in the log) should remain noted.
        *   **Robust Verification Strategies:** For both user and AI, targeted `grep_search` or precise `edit_file` attempts (where "no changes made" is informative) are more reliable for confirming specific states than visual inspection of potentially summarized large file reads in the log, especially when line numbers are volatile. These should be used by the AI to challenge its own internal state if contradictions arise.

---

*(More issues can be added here as they are identified for this or other documents.)*
