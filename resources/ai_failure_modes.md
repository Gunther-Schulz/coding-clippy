# AI Failure Modes in Software Development: Analysis & Mitigation

## Overview

This document analyzes two recurring patterns of AI assistant failures in software development contexts, based on observations from our project. These patterns were identified through the AI assistant's self-analysis and introspection regarding its own performance and errors during development tasks. Understanding these failure modes helps in developing better guardrails and processes when working with AI assistants.

## Failure Mode 1: Code Generation Verification Failures

### Context

During pair programming sessions, the AI assistant (Gemini 2.5 Pro) has introduced errors while applying code edits.
*   **Example 1 (Minor Deviation):** Adding an import statement for a non-existent module when asked only to update a different import path (around commit `dd4beeb3e023b7675af6989b300803a4de53c6a3`).
*   **Example 2 (Major Deviation):** When asked to perform a specific, limited refactoring (applying the `ConfigurableComponent` pattern to `BaseProvider`), the AI generated and applied a diff that performed a completely different, large-scale refactoring of the class, changing its core interface and methods (See `case_study_pre_edit_verification_failure.md`).

### Root Cause Analysis

1.  **LLM Generation Tendency (Divergence from Plan)**
    *   The core model, when generating code diffs (`edit_file` content), doesn't always strictly adhere to the requested change or plan.
    *   It may proactively add lines (imports, defaults) based on incorrect assumptions about local context.
    *   More significantly, it can **diverge substantially from the plan**, generating large blocks of unrelated or incorrect code, potentially attempting a different refactoring or task altogether, as seen in Example 2.
    *   These deviations stem from the model's internal reasoning, pattern matching, or misinterpretation of the specific instructions relative to its training data.

2.  **Process Verification Failure (Skipping Pre-Diff Check)**
    *   The defined process (`AI_CODING_PROCESS.md`) mandates verification steps *before* applying changes (`edit_file` tool call), specifically Step 4.A which focuses on reviewing the *proposed code diff* generated by the AI.
    *   A critical failure occurs when this pre-edit diff verification (Step 4.A) is skipped or performed superficially.
    *   **Conflating Plan vs. Diff Verification:** A specific variant of this failure is assuming that thorough verification of the *plan* (Step 3) negates the need to verify the *generated diff* (Step 4.A). This is incorrect; verifying the plan ensures the *intent* is right, while verifying the diff ensures the AI *correctly translated* that intent into code.
    *   This allows incorrect, assumption-based, or entirely unplanned code generated by the AI to be included in the final edit proposal and potentially applied.

### Mitigation

*   **Mandatory Pre-Edit Diff Verification (Step 4.A):** Strengthen and rigorously enforce the verification phase (Step 4.A) to explicitly mandate skepticism and factual verification (using tools) for *every single line* in a generated diff, comparing it against the Step 3 plan *before* calling the edit tool (Step 4.B).
*   **Distinguish Plan vs. Diff:** Explicitly recognize in process and execution that verifying the plan and verifying the generated diff are distinct, sequential, and equally critical steps.
*   **Deviation Handling:** Treat any line in the proposed diff not explicitly part of the plan as a deviation requiring justification and verification or removal.

## Failure Mode 2: Implementation Planning Failures

### Context

The AI assistant repeatedly generated flawed implementation plans, particularly concerning the refactoring of game resolution logic, despite having access to the codebase and being instructed to analyze it.

### Root Cause Analysis

1. **Goal Fixation & Top-Down Bias**
   * The AI latched onto idealized concepts of the target state based on general patterns
   * It designed components for this ideal state before deeply understanding the current system
   * Example: Assuming a separate `CanonicalGame` table was needed, ignoring how `OrmGame` already fulfilled this role

2. **Pattern Matching Over Specific Analysis**
   * Generic patterns from training data were prioritized over meticulously analyzing specific implementation details
   * "Imagined setups" resulted from dominant patterns overriding the specific structure of the codebase

3. **Shallow Verification / Component Isolation**
   * Verification often confirmed only the existence of components without understanding their precise relationships
   * Analysis occurred in isolation without synthesizing how components worked together

4. **Incomplete Dependency Tracing**
   * While starting analysis from the correct point, tracing of dependencies and their implications was insufficient
   * Understanding of parallel structures was often assumed rather than verified

### Correct Process Flow

1. Start at the entry point (e.g., service layer)
2. Identify core dependencies
3. Analyze current methods AND the repository methods they call
4. Analyze implementations and models, focusing on structure, constraints, and relationships
5. **Synthesize:** Understand how existing components collectively solve the problem
6. **Plan:** Base all changes on interacting with the existing structure

## Common Patterns & Unified Mitigation

Both failure modes share common characteristics:

1. **Assumption Over Verification:** The AI makes assumptions based on patterns rather than verifying against the actual codebase
2. **Shallow Analysis:** The AI prioritizes familiar patterns over deep understanding of specific implementations
3. **Process Shortcuts:** Critical verification steps are skipped or performed superficially
4. **Superficial Adherence / 'Checking Boxes':** The AI performs mandated process steps (like verification) but without sufficient depth, critical evaluation, or consideration of side-effects, merely fulfilling the requirement formally.

### Unified Mitigation Strategy

1. **Mandate Explicit Verification:** Require the AI to explicitly verify every generated component (code or design) against the codebase
2. **"Show Your Work" Principle:** The AI must demonstrate how it traced dependencies and reached conclusions, citing specific evidence from the code or documentation.
3. **Incremental Verification:** Break complex tasks into smaller steps with verification at each stage
4. **Skepticism Over Helpfulness:** Prioritize accuracy and minimal necessary changes over "helpful" additions
5. **Reality Checks:** Periodically force the AI to compare its mental model with the actual codebase state
6. **Emphasize Critical Evaluation:** Procedures must explicitly require not just performing checks, but confirming critical evaluation beyond surface-level execution, including considering potential side-effects and unintended consequences.
